{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f07513",
   "metadata": {},
   "source": [
    "## Open-Model-Zoo Downloader\n",
    "\n",
    "#### Just type the help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e576f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: omz_downloader [-h] [--name PAT[,PAT...]] [--list FILE.LST] [--all]\n",
      "                      [--print_all] [--precisions PREC[,PREC...]] [-o DIR]\n",
      "                      [--cache_dir DIR] [--num_attempts N]\n",
      "                      [--progress_format {text,json}] [-j N]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --name PAT[,PAT...]   download only models whose names match at least one of\n",
      "                        the specified patterns\n",
      "  --list FILE.LST       download only models whose names match at least one of\n",
      "                        the patterns in the specified file\n",
      "  --all                 download all available models\n",
      "  --print_all           print all available models\n",
      "  --precisions PREC[,PREC...]\n",
      "                        download only models with the specified precisions\n",
      "                        (actual for DLDT networks); specify one or more of:\n",
      "                        FP32-INT8,FP16-INT1,FP32,FP16-INT8,FP16,FP32-INT1\n",
      "  -o DIR, --output_dir DIR\n",
      "                        path where to save models\n",
      "  --cache_dir DIR       directory to use as a cache for downloaded files\n",
      "  --num_attempts N      attempt each download up to N times\n",
      "  --progress_format {text,json}\n",
      "                        which format to use for progress reporting\n",
      "  -j N, --jobs N        how many downloads to perform concurrently\n"
     ]
    }
   ],
   "source": [
    "! omz_downloader -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792c845",
   "metadata": {},
   "source": [
    "#### Print all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1af446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sphereface\n",
      "aclnet\n",
      "aclnet-int8\n",
      "action-recognition-0001\n",
      "age-gender-recognition-retail-0013\n",
      "alexnet\n",
      "anti-spoof-mn3\n",
      "asl-recognition-0004\n",
      "background-matting-mobilenetv2\n",
      "bert-base-ner\n",
      "bert-large-uncased-whole-word-masking-squad-0001\n",
      "bert-large-uncased-whole-word-masking-squad-emb-0001\n",
      "bert-large-uncased-whole-word-masking-squad-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0001\n",
      "bert-small-uncased-whole-word-masking-squad-0002\n",
      "bert-small-uncased-whole-word-masking-squad-emb-int8-0001\n",
      "bert-small-uncased-whole-word-masking-squad-int8-0002\n",
      "brain-tumor-segmentation-0001\n",
      "brain-tumor-segmentation-0002\n",
      "caffenet\n",
      "cocosnet\n",
      "colorization-siggraph\n",
      "colorization-v2\n",
      "common-sign-language-0001\n",
      "common-sign-language-0002\n",
      "convnext-tiny\n",
      "ctdet_coco_dlav0_512\n",
      "ctpn\n",
      "deblurgan-v2\n",
      "deeplabv3\n",
      "densenet-121\n",
      "densenet-121-tf\n",
      "detr-resnet50\n",
      "dla-34\n",
      "driver-action-recognition-adas-0002\n",
      "drn-d-38\n",
      "efficientdet-d0-tf\n",
      "efficientdet-d1-tf\n",
      "efficientnet-b0\n",
      "efficientnet-b0-pytorch\n",
      "efficientnet-v2-b0\n",
      "efficientnet-v2-s\n",
      "emotions-recognition-retail-0003\n",
      "erfnet\n",
      "f3net\n",
      "face-detection-0200\n",
      "face-detection-0202\n",
      "face-detection-0204\n",
      "face-detection-0205\n",
      "face-detection-0206\n",
      "face-detection-adas-0001\n",
      "face-detection-retail-0004\n",
      "face-detection-retail-0005\n",
      "face-detection-retail-0044\n",
      "face-recognition-resnet100-arcface-onnx\n",
      "face-reidentification-retail-0095\n",
      "faceboxes-pytorch\n",
      "facenet-20180408-102900\n",
      "facial-landmarks-35-adas-0002\n",
      "facial-landmarks-98-detection-0001\n",
      "fast-neural-style-mosaic-onnx\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001\n",
      "faster_rcnn_inception_resnet_v2_atrous_coco\n",
      "faster_rcnn_resnet50_coco\n",
      "fastseg-large\n",
      "fastseg-small\n",
      "fbcnn\n",
      "fcrn-dp-nyu-depth-v2-tf\n",
      "formula-recognition-medium-scan-0001\n",
      "formula-recognition-polynomials-handwritten-0001\n",
      "forward-tacotron\n",
      "gaze-estimation-adas-0002\n",
      "gmcnn-places2-tf\n",
      "googlenet-v1\n",
      "googlenet-v1-tf\n",
      "googlenet-v2\n",
      "googlenet-v2-tf\n",
      "googlenet-v3\n",
      "googlenet-v3-pytorch\n",
      "googlenet-v4-tf\n",
      "gpt-2\n",
      "handwritten-english-recognition-0001\n",
      "handwritten-japanese-recognition-0001\n",
      "handwritten-score-recognition-0003\n",
      "handwritten-simplified-chinese-recognition-0001\n",
      "hbonet-0.25\n",
      "hbonet-1.0\n",
      "head-pose-estimation-adas-0001\n",
      "higher-hrnet-w32-human-pose-estimation\n",
      "horizontal-text-detection-0001\n",
      "hrnet-v2-c1-segmentation\n",
      "human-pose-estimation-0001\n",
      "human-pose-estimation-0005\n",
      "human-pose-estimation-0006\n",
      "human-pose-estimation-0007\n",
      "human-pose-estimation-3d-0001\n",
      "hybrid-cs-model-mri\n",
      "i3d-rgb-tf\n",
      "icnet-camvid-ava-0001\n",
      "icnet-camvid-ava-sparse-30-0001\n",
      "icnet-camvid-ava-sparse-60-0001\n",
      "image-retrieval-0001\n",
      "inception-resnet-v2-tf\n",
      "instance-segmentation-person-0007\n",
      "instance-segmentation-security-0002\n",
      "instance-segmentation-security-0091\n",
      "instance-segmentation-security-0228\n",
      "instance-segmentation-security-1039\n",
      "instance-segmentation-security-1040\n",
      "landmarks-regression-retail-0009\n",
      "levit-128s\n",
      "license-plate-recognition-barrier-0001\n",
      "license-plate-recognition-barrier-0007\n",
      "machine-translation-nar-de-en-0002\n",
      "machine-translation-nar-en-de-0002\n",
      "machine-translation-nar-en-ru-0002\n",
      "machine-translation-nar-ru-en-0002\n",
      "mask_rcnn_inception_resnet_v2_atrous_coco\n",
      "mask_rcnn_resnet50_atrous_coco\n",
      "midasnet\n",
      "mixnet-l\n",
      "mobilefacedet-v1-mxnet\n",
      "mobilenet-ssd\n",
      "mobilenet-v1-0.25-128\n",
      "mobilenet-v1-1.0-224\n",
      "mobilenet-v1-1.0-224-tf\n",
      "mobilenet-v2\n",
      "mobilenet-v2-1.0-224\n",
      "mobilenet-v2-1.4-224\n",
      "mobilenet-v2-pytorch\n",
      "mobilenet-v3-large-1.0-224-paddle\n",
      "mobilenet-v3-large-1.0-224-tf\n",
      "mobilenet-v3-small-1.0-224-paddle\n",
      "mobilenet-v3-small-1.0-224-tf\n",
      "mobilenet-yolo-v4-syg\n",
      "modnet-photographic-portrait-matting\n",
      "modnet-webcam-portrait-matting\n",
      "mozilla-deepspeech-0.6.1\n",
      "mozilla-deepspeech-0.8.2\n",
      "mtcnn\n",
      "nanodet-m-1.5x-416\n",
      "nanodet-plus-m-1.5x-416\n",
      "netvlad-tf\n",
      "nfnet-f0\n",
      "noise-suppression-denseunet-ll-0001\n",
      "noise-suppression-poconetlike-0001\n",
      "ocrnet-hrnet-w48-paddle\n",
      "octave-resnet-26-0.25\n",
      "open-closed-eye-0001\n",
      "pedestrian-and-vehicle-detector-adas-0001\n",
      "pedestrian-detection-adas-0002\n",
      "pelee-coco\n",
      "person-attributes-recognition-crossroad-0230\n",
      "person-attributes-recognition-crossroad-0234\n",
      "person-attributes-recognition-crossroad-0238\n",
      "person-detection-0106\n",
      "person-detection-0200\n",
      "person-detection-0201\n",
      "person-detection-0202\n",
      "person-detection-0203\n",
      "person-detection-0301\n",
      "person-detection-0302\n",
      "person-detection-0303\n",
      "person-detection-action-recognition-0005\n",
      "person-detection-action-recognition-0006\n",
      "person-detection-action-recognition-teacher-0002\n",
      "person-detection-asl-0001\n",
      "person-detection-raisinghand-recognition-0001\n",
      "person-detection-retail-0002\n",
      "person-detection-retail-0013\n",
      "person-reidentification-retail-0277\n",
      "person-reidentification-retail-0286\n",
      "person-reidentification-retail-0287\n",
      "person-reidentification-retail-0288\n",
      "person-vehicle-bike-detection-2000\n",
      "person-vehicle-bike-detection-2001\n",
      "person-vehicle-bike-detection-2002\n",
      "person-vehicle-bike-detection-2003\n",
      "person-vehicle-bike-detection-2004\n",
      "person-vehicle-bike-detection-crossroad-0078\n",
      "person-vehicle-bike-detection-crossroad-1016\n",
      "person-vehicle-bike-detection-crossroad-yolov3-1020\n",
      "product-detection-0001\n",
      "pspnet-pytorch\n",
      "quartznet-15x5-en\n",
      "regnetx-3.2gf\n",
      "repvgg-a0\n",
      "repvgg-b1\n",
      "repvgg-b3\n",
      "resnest-50-pytorch\n",
      "resnet-18-pytorch\n",
      "resnet-34-pytorch\n",
      "resnet-50-pytorch\n",
      "resnet-50-tf\n",
      "resnet18-xnor-binary-onnx-0001\n",
      "resnet50-binary-0001\n",
      "retinaface-resnet50-pytorch\n",
      "retinanet-tf\n",
      "rexnet-v1-x1.0\n",
      "rfcn-resnet101-coco-tf\n",
      "road-segmentation-adas-0001\n",
      "robust-video-matting-mobilenetv3\n",
      "se-inception\n",
      "se-resnet-50\n",
      "se-resnext-50\n",
      "semantic-segmentation-adas-0001\n",
      "shufflenet-v2-x0.5\n",
      "shufflenet-v2-x1.0\n",
      "single-human-pose-estimation-0001\n",
      "single-image-super-resolution-1032\n",
      "single-image-super-resolution-1033\n",
      "smartlab-action-recognition-0001\n",
      "smartlab-object-detection-0001\n",
      "smartlab-object-detection-0002\n",
      "smartlab-object-detection-0003\n",
      "smartlab-object-detection-0004\n",
      "smartlab-sequence-modelling-0001\n",
      "smartlab-sequence-modelling-0002\n",
      "squeezenet1.0\n",
      "squeezenet1.1\n",
      "ssd-resnet34-1200-onnx\n",
      "ssd300\n",
      "ssd512\n",
      "ssd_mobilenet_v1_coco\n",
      "ssd_mobilenet_v1_fpn_coco\n",
      "ssdlite_mobilenet_v2\n",
      "swin-tiny-patch4-window7-224\n",
      "t2t-vit-14\n",
      "text-detection-0003\n",
      "text-detection-0004\n",
      "text-image-super-resolution-0001\n",
      "text-recognition-0012\n",
      "text-recognition-0014\n",
      "text-recognition-0015\n",
      "text-recognition-0016\n",
      "text-recognition-resnet-fc\n",
      "text-spotting-0005\n",
      "text-to-speech-en-0001\n",
      "text-to-speech-en-multi-0001\n",
      "time-series-forecasting-electricity-0001\n",
      "ultra-lightweight-face-detection-rfb-320\n",
      "ultra-lightweight-face-detection-slim-320\n",
      "unet-camvid-onnx-0001\n",
      "vehicle-attributes-recognition-barrier-0039\n",
      "vehicle-attributes-recognition-barrier-0042\n",
      "vehicle-detection-0200\n",
      "vehicle-detection-0201\n",
      "vehicle-detection-0202\n",
      "vehicle-detection-adas-0002\n",
      "vehicle-license-plate-detection-barrier-0106\n",
      "vehicle-license-plate-detection-barrier-0123\n",
      "vehicle-reid-0001\n",
      "vgg16\n",
      "vgg19\n",
      "vitstr-small-patch16-224\n",
      "wav2vec2-base\n",
      "wavernn\n",
      "weld-porosity-detection-0001\n",
      "yolact-resnet50-fpn-pytorch\n",
      "yolo-v1-tiny-tf\n",
      "yolo-v2-ava-0001\n",
      "yolo-v2-ava-sparse-35-0001\n",
      "yolo-v2-ava-sparse-70-0001\n",
      "yolo-v2-tf\n",
      "yolo-v2-tiny-ava-0001\n",
      "yolo-v2-tiny-ava-sparse-30-0001\n",
      "yolo-v2-tiny-ava-sparse-60-0001\n",
      "yolo-v2-tiny-tf\n",
      "yolo-v2-tiny-vehicle-detection-0001\n",
      "yolo-v3-onnx\n",
      "yolo-v3-tf\n",
      "yolo-v3-tiny-onnx\n",
      "yolo-v3-tiny-tf\n",
      "yolo-v4-tf\n",
      "yolo-v4-tiny-tf\n",
      "yolof\n",
      "yolox-tiny\n"
     ]
    }
   ],
   "source": [
    "! omz_downloader --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688109e",
   "metadata": {},
   "source": [
    "#### Download the face detection for ADAS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4775ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading face-detection-adas-0001 ||################\n",
      "\n",
      "========== Downloading C:\\Users\\gtamir\\Vision-with-OpenVino\\3. Get the model\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.xml\n",
      "... 100%, 304 KB, 1083 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading C:\\Users\\gtamir\\Vision-with-OpenVino\\3. Get the model\\intel\\face-detection-adas-0001\\FP16\\face-detection-adas-0001.bin\n",
      "... 49%, 1024 KB, 2337 KB/s, 0 seconds passed\n",
      "... 99%, 2048 KB, 3447 KB/s, 0 seconds passed\n",
      "... 100%, 2056 KB, 3371 KB/s, 0 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! omz_downloader --name face-detection-adas-0001 --precision FP16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb52b60",
   "metadata": {},
   "source": [
    "#### Convert the model to OpenVINO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52448608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Skipping face-detection-adas-0001 (no conversions defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! omz_converter --name face-detection-adas-0001 --precision FP16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fbac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
